{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from scipy.spatial.distance import cdist \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# Suppress heavy TF logs and warnings for cleaner output\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a66d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your specific function data here.\n",
    "# Ensure these files exist in the directory you are running the notebook from.\n",
    "\n",
    "week = 'week13' \n",
    "# Adjust this path if your folder structure is different in the repo\n",
    "base_path = './Capstone_Data/' + week + '_data/function_8/'\n",
    "\n",
    "try:\n",
    "    data_x = np.load(base_path + 'initial_inputs.npy')\n",
    "    data_y = np.load(base_path + 'initial_outputs.npy')\n",
    "    print(f\"Data Loaded Successfully: X shape {data_x.shape}, Y shape {data_y.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find .npy files at {base_path}\")\n",
    "    print(\"Please check your directory structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "--- Helper Function: Latin Hypercube Sampling ---\n",
    "def get_latin_hypercube_samples(bounds, n_samples, seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    seed_val = rng.randint(0, 10000)\n",
    "    sampler = qmc.LatinHypercube(d=len(bounds), optimization=\"random-cd\", seed=seed_val)\n",
    "    samples = sampler.random(n_samples)\n",
    "    l_bounds = bounds[:, 0]\n",
    "    u_bounds = bounds[:, 1]\n",
    "    samples_scaled = qmc.scale(samples, l_bounds, u_bounds)\n",
    "    return samples_scaled\n",
    "\n",
    "# --- 1. Define Keras Model ---\n",
    "def create_model(n_dim, learning_rate=1e-3):\n",
    "    model = Sequential([\n",
    "        Input(shape=(n_dim,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# --- 2. Deep Ensemble Class ---\n",
    "class DeepEnsemble:\n",
    "    def __init__(self, n_models, n_dim):\n",
    "        self.n_models = n_models\n",
    "        self.n_dim = n_dim\n",
    "        self.models = [create_model(n_dim) for _ in range(n_models)]\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size, validation_split=0.2, verbose=0):\n",
    "        # Callbacks for robust training\n",
    "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        callbacks = [early_stopper, lr_scheduler]\n",
    "\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Bagging: Sample with replacement to induce diversity\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            model.fit(X_sample, y_sample, epochs=epochs, batch_size=batch_size,\n",
    "                      shuffle=True, verbose=verbose, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        all_preds = [model.predict(X_test, verbose=0) for model in self.models]\n",
    "        stacked_preds = np.stack(all_preds, axis=0)\n",
    "        mu = np.mean(stacked_preds, axis=0)\n",
    "        std = np.std(stacked_preds, axis=0)\n",
    "        return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2188f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. BBO Function with PCA and Spatially Aware Filtering ---\n",
    "def BBO_Ensemble_PCA(X, Y, beta, n_candidates, seed, n_models, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Propose next point using Deep Ensemble with Optional PCA and \n",
    "    spatially-aware acquisition penalties.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Y = np.asarray(Y, dtype=float).reshape(-1, 1)\n",
    "    n_dim = X.shape[1]\n",
    "\n",
    "    # --- PCA LOGIC ---\n",
    "    # Apply PCA only if dimensions are high (e.g., >= 5) and sufficient data exists\n",
    "    use_pca = (n_dim >= 5) and (X.shape[0] > n_dim)\n",
    "\n",
    "    if use_pca:\n",
    "        n_components = 3\n",
    "        print(f\"Applying PCA: Reducing {n_dim}D -> {n_components}D\")\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_transformed = pca.fit_transform(X)\n",
    "        opt_dim = n_components\n",
    "    else:\n",
    "        print(f\"Skipping PCA: optimizing in original {n_dim}D space\")\n",
    "        X_transformed = X\n",
    "        opt_dim = n_dim\n",
    "\n",
    "    # 1. Scale Data\n",
    "    x_scaler = StandardScaler()\n",
    "    X_scaled = x_scaler.fit_transform(X_transformed)\n",
    "    y_scaler = StandardScaler()\n",
    "    Y_scaled = y_scaler.fit_transform(Y)\n",
    "\n",
    "    # 2. Train Ensemble\n",
    "    print(f\"Training Deep Ensemble ({n_models} models)...\")\n",
    "    ensemble = DeepEnsemble(n_models=n_models, n_dim=opt_dim)\n",
    "    ensemble.fit(X_scaled, Y_scaled, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # 3. Generate Candidate Grid\n",
    "    if use_pca:\n",
    "        # Sample in latent space (Gaussian)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        grid_latent = rng.standard_normal(size=(n_candidates, opt_dim))\n",
    "        grid_scaled = grid_latent \n",
    "    else:\n",
    "        # Sample in original space (Latin Hypercube)\n",
    "        bounds = np.array([[0.0, 1.0]] * n_dim)\n",
    "        grid = get_latin_hypercube_samples(bounds, n_candidates, seed)\n",
    "        grid_scaled = x_scaler.transform(grid)\n",
    "\n",
    "    # 4. Predict\n",
    "    mu_scaled, std_scaled = ensemble.predict(grid_scaled)\n",
    "\n",
    "    # 5. Inverse Scale Predictions\n",
    "    mu = y_scaler.inverse_transform(mu_scaled)\n",
    "    std = std_scaled * y_scaler.scale_\n",
    "\n",
    "    # 6. Calculate UCB Acquisition\n",
    "    ucb = mu.ravel() + beta * std.ravel()\n",
    "\n",
    "    # --- Spatially Aware Filtering ---\n",
    "    # A. Domain Constraints\n",
    "    if use_pca:\n",
    "        grid_original_space = pca.inverse_transform(x_scaler.inverse_transform(grid_scaled))\n",
    "        # Discard points outside valid bounds [0,1]\n",
    "        mask = np.all((grid_original_space >= 0.0) & (grid_original_space <= 1.0), axis=1)\n",
    "        \n",
    "        if np.sum(mask) > 100:\n",
    "            candidates_X = grid_original_space[mask]\n",
    "            ucb = ucb[mask]\n",
    "        else:\n",
    "            candidates_X = np.clip(grid_original_space, 0.0, 1.0)\n",
    "    else:\n",
    "        candidates_X = grid\n",
    "\n",
    "    # B. Local Penalties (Penalize regions known to be poor)\n",
    "    nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(candidates_X)\n",
    "    \n",
    "    neighbor_Y = Y[indices].squeeze() \n",
    "    local_mean_Y = np.mean(neighbor_Y, axis=1)\n",
    "    global_mean_Y = np.mean(Y)\n",
    "    \n",
    "    # Punishment factor for bad neighborhoods\n",
    "    scaling_factor = 2.0\n",
    "    penalty = np.maximum(0, global_mean_Y - local_mean_Y) * scaling_factor\n",
    "    ucb_adjusted = ucb - penalty\n",
    "\n",
    "    # C. \"Too Close\" Filter (Prevent resampling)\n",
    "    min_dist_to_known = np.min(distances, axis=1)\n",
    "    ucb_adjusted[min_dist_to_known < 1e-4] = -np.inf\n",
    "\n",
    "    # 7. Select Best\n",
    "    best_idx = int(np.argmax(ucb_adjusted))\n",
    "    x_next = candidates_X[best_idx]\n",
    "\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb86792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execution ---\n",
    "\n",
    "# Parameters\n",
    "BETA = 0.05        # Low exploration weight for exploitation\n",
    "SEARCH_SIZE = 10000\n",
    "N_MODELS = 20      # Number of models in ensemble\n",
    "EPOCHS = 250       \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Note: Ensure data_x and data_y are loaded in Cell 2 before running this!\n",
    "if 'data_x' in locals() and 'data_y' in locals():\n",
    "    print(f\"--- Running Optimization on Function ---\")\n",
    "    next_point = BBO_Ensemble_PCA(data_x, data_y, BETA, SEARCH_SIZE, SEED,\n",
    "                                   n_models=N_MODELS,\n",
    "                                   epochs=EPOCHS,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Formatting Output\n",
    "    next_point_rounded = np.round(next_point, 6)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Next Query Point Suggested: \\n{next_point_rounded}\")\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Error: data_x and data_y not defined. Please check Cell 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c156cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6aaa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e535cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d260b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd99c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
